{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HANConv, DeepGraphInfomax, Linear\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.utils import dense_to_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**1. Load and Process Data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1. 데이터 로드\n",
    "ppi_df = pd.read_csv(\"../analysis/assets/module_assignment_network/ppi_mat.tsv.gz\", sep=\"\\t\", index_col=\"gene_name\")\n",
    "tom_df = pd.read_csv(\"../analysis/assets/module_assignment_network/tom_mat.tsv.gz\", sep=\"\\t\", index_col=\"gene_name\")\n",
    "go_hierarchy_df = pd.read_csv(\"../analysis/assets/module_assignment_network/go_hierarchy.txt.gz\", sep=\"\\t\", index_col=0)\n",
    "gene_to_go_df = pd.read_csv(\"../analysis/assets/module_assignment_network/gene_to_go.tsv.gz\", sep=\"\\t\")\n",
    "module_df = pd.read_csv(\"../analysis/assets/wgcna/full_module_scores.txt.gz\", sep=\"\\t\")\n",
    "map_cis_df = pd.read_csv(\"../analysis/assets/module_assignment_network//eQTL_sizes.tsv.gz\", sep=\"\\t\")\n",
    "\n",
    "#  2. Grey module 유전자 제거\n",
    "valid_genes = module_df[module_df[\"module\"] != \"grey\"][\"gene_name\"].unique()\n",
    "eQTL_genes  = map_cis_df[\"phenotype_id\"].unique()\n",
    "\n",
    "#  3. PPI, TOM, GO 어노테이션과 공통된 유전자 추출\n",
    "common_genes = sorted(set(ppi_df.index) & set(tom_df.index) & set(gene_to_go_df['Gene']) & set(valid_genes) & set(eQTL_genes))\n",
    "\n",
    "#  4. module_df를 common_genes 순서로 정렬\n",
    "module_df = module_df[module_df[\"gene_name\"].isin(common_genes)].set_index(\"gene_name\").reindex(common_genes).reset_index()\n",
    "module_to_idx = {mod: i for i, mod in enumerate(sorted(module_df[\"module\"].unique()))}  # Module 인덱스 매핑\n",
    "\n",
    "map_cis_df = map_cis_df[map_cis_df[\"phenotype_id\"].isin(common_genes)].set_index(\"phenotype_id\").reindex(common_genes).reset_index()\n",
    "\n",
    "#  5. PPI 및 TOM 데이터에서 common_genes만 선택\n",
    "ppi_arr = ppi_df.loc[common_genes, common_genes]\n",
    "tom_arr = tom_df.loc[common_genes, common_genes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 module_features 와 concat (num_genes, num_modules + 1)\n",
    "eqtl_features = map_cis_df.iloc[:,1:].to_numpy()\n",
    "# eQTL effect size 벡터 추가 (num_genes, 1) shape\n",
    "eqtl_tensor = torch.tensor(eqtl_features, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) One-hot encoding\n",
    "num_genes = len(common_genes)  #  수정: common_genes 기준으로 갯수 설정\n",
    "unique_modules = sorted(module_df[module_df[\"module\"] != \"grey\"][\"module\"].unique())\n",
    "num_modules = len(unique_modules)\n",
    "one_hot_matrix = np.zeros((num_genes, num_modules))\n",
    "for i, mod in enumerate(module_df[\"module\"]):\n",
    "    one_hot_matrix[i, module_to_idx[mod]] = 1\n",
    "module_features = torch.tensor(one_hot_matrix, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_to_go_df  = gene_to_go_df[ gene_to_go_df['Gene'].isin(common_genes) ]\n",
    "go_terms_with_genes = set(gene_to_go_df[\"GO_Term\"])\n",
    "filtered_go_hierarchy_df = go_hierarchy_df[\n",
    "    (go_hierarchy_df[\"GO_term\"].isin(go_terms_with_genes)) | \n",
    "    (go_hierarchy_df[\"Parent_GO_term\"].isin(go_terms_with_genes))\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GO 어노테이션 및 계층 구조에 등장하는 GO term들을 모두 포함\n",
    "go_terms_from_annotation = set(gene_to_go_df['GO_Term'])\n",
    "go_terms_from_hierarchy = set(filtered_go_hierarchy_df['GO_term']) | set(filtered_go_hierarchy_df['Parent_GO_term'])\n",
    "all_go_terms = sorted(go_terms_from_annotation.union(go_terms_from_hierarchy))\n",
    "num_go_nodes = len(all_go_terms)\n",
    "go_term_to_idx = {term: i for i, term in enumerate(all_go_terms)}\n",
    "gene_to_idx = {gene: i for i, gene in enumerate(common_genes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [A] gene → GO edge 구축 (어노테이션 정보)\n",
    "gene_go_source = []\n",
    "gene_go_target = []\n",
    "for _, row in gene_to_go_df.iterrows():\n",
    "    gene = row['Gene']\n",
    "    go_term = row['GO_Term']\n",
    "    if gene in gene_to_idx and go_term in go_term_to_idx:\n",
    "        gene_go_source.append(gene_to_idx[gene])\n",
    "        gene_go_target.append(go_term_to_idx[go_term])\n",
    "# edge_index shape: [2, num_edges]\n",
    "gene_go_edge_index = torch.tensor([gene_go_source, gene_go_target], dtype=torch.long)\n",
    "\n",
    "# [B] GO → gene 역방향 edge (반대 방향 추가)\n",
    "go_gene_edge_index = torch.tensor([gene_go_target, gene_go_source], dtype=torch.long)\n",
    "\n",
    "# [C] GO 계층 구조 edge 구축: child (GO_term) → parent (Parent_GO_term)\n",
    "go_hierarchy_source = []\n",
    "go_hierarchy_target = []\n",
    "for _, row in filtered_go_hierarchy_df.iterrows():\n",
    "    child = row['GO_term']\n",
    "    parent = row['Parent_GO_term']\n",
    "    if child in go_term_to_idx and parent in go_term_to_idx:\n",
    "        go_hierarchy_source.append(go_term_to_idx[child])\n",
    "        go_hierarchy_target.append(go_term_to_idx[parent])\n",
    "go_hierarchy_edge_index = torch.tensor([go_hierarchy_source, go_hierarchy_target], dtype=torch.long)\n",
    "\n",
    "# (선택사항) GO 계층 구조의 역방향 edge: parent → child\n",
    "go_hierarchy_rev_edge_index = torch.tensor([go_hierarchy_target, go_hierarchy_source], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Apply Edge Thresholding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining PPI edges: 44030\n",
      "Remaining TOM edges: 143192\n",
      "Remaining intersected edges: 7676\n"
     ]
    }
   ],
   "source": [
    "PPI_THRESHOLD = 700   # STRING database recommends 700+ for high-confidence interactions\n",
    "TOM_THRESHOLD = 0.10  # Keep only strong TOM connections\n",
    "\n",
    "# Filter edges based on threshold\n",
    "ppi_filtered = np.where(ppi_arr > PPI_THRESHOLD, ppi_arr, 0)\n",
    "tom_filtered = np.where(tom_arr > TOM_THRESHOLD, tom_arr, 0)\n",
    "\n",
    "# Keep only edges that pass both PPI and TOM thresholds\n",
    "combined_mask = (ppi_filtered > 0) & (tom_filtered > 0) \n",
    "#ppi_filtered = np.where(combined_mask, ppi_filtered, 0)\n",
    "#tom_filtered = np.where(combined_mask, tom_filtered, 0)\n",
    "\n",
    "# Print summary of remaining edges\n",
    "print(f\"Remaining PPI edges: {np.count_nonzero(ppi_filtered)}\")\n",
    "print(f\"Remaining TOM edges: {np.count_nonzero(tom_filtered)}\")\n",
    "print(f\"Remaining intersected edges: {np.count_nonzero(combined_mask)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Convert to PyTorch Format and Heterogenous Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert adjacency matrices to tensors\n",
    "ppi_tensor = torch.tensor(ppi_filtered, dtype=torch.float)\n",
    "tom_tensor = torch.tensor(tom_filtered, dtype=torch.float)\n",
    "\n",
    "# Convert dense adjacency matrices to sparse format (edge lists)\n",
    "ppi_edge_index, ppi_edge_weight = dense_to_sparse(ppi_tensor)\n",
    "tom_edge_index, tom_edge_weight = dense_to_sparse(tom_tensor)\n",
    "\n",
    "data = HeteroData()\n",
    "\n",
    "# Assign one-hot encoded node features (replace with real features if available)\n",
    "num_nodes = len(common_genes)\n",
    "#data[\"gene\"].x = torch.eye(num_nodes)  # Identity matrix as placeholder features\n",
    "#data[\"gene\"].x = torch.cat([data[\"gene\"].x, module_features], dim=1)\n",
    "gene_features = torch.cat([module_features, eqtl_tensor], dim=1)\n",
    "data[\"gene\"].x = gene_features\n",
    "\n",
    "# Store gene names\n",
    "data[\"gene\"].names = common_genes\n",
    "\n",
    "# [ii] go 노드: GO term 개수에 맞는 one-hot (임시) feature 사용\n",
    "num_go_features = 64\n",
    "\n",
    "data[\"go\"].x = torch.nn.Embedding(num_go_nodes, num_go_features).weight \n",
    "data[\"go\"].names = all_go_terms\n",
    "\n",
    "\n",
    "# Assign edges to the graph\n",
    "data[\"gene\", \"PPI\", \"gene\"].edge_index = ppi_edge_index\n",
    "data[\"gene\", \"PPI\", \"gene\"].edge_attr = ppi_edge_weight.unsqueeze(-1) \n",
    "\n",
    "data[\"gene\", \"TOM\", \"gene\"].edge_index = tom_edge_index\n",
    "data[\"gene\", \"TOM\", \"gene\"].edge_attr = tom_edge_weight.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "노드 타입: ['gene', 'go']\n",
      "엣지 타입: [('gene', 'PPI', 'gene'), ('gene', 'TOM', 'gene'), ('gene', 'has_GO', 'go'), ('go', 'is_a', 'go')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  gene={\n",
       "    x=[2615, 12],\n",
       "    names=[2615],\n",
       "  },\n",
       "  go={\n",
       "    x=[23934, 64],\n",
       "    names=[23934],\n",
       "  },\n",
       "  (gene, PPI, gene)={\n",
       "    edge_index=[2, 44030],\n",
       "    edge_attr=[44030, 1],\n",
       "  },\n",
       "  (gene, TOM, gene)={\n",
       "    edge_index=[2, 143192],\n",
       "    edge_attr=[143192, 1],\n",
       "  },\n",
       "  (gene, has_GO, go)={\n",
       "    edge_index=[2, 146545],\n",
       "    edge_attr=[146545, 1],\n",
       "  },\n",
       "  (go, is_a, go)={\n",
       "    edge_index=[2, 29365],\n",
       "    edge_attr=[29365, 1],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"gene\", \"has_GO\", \"go\"].edge_index = gene_go_edge_index \n",
    "data[\"gene\", \"has_GO\", \"go\"].edge_attr = torch.ones(gene_go_edge_index.size(1),1) \n",
    "\n",
    "#data[\"go\", \"rev_has_GO\", \"gene\"].edge_index = go_gene_edge_index\n",
    "#data[\"go\", \"rev_has_GO\", \"gene\"].edge_attr = torch.ones(go_gene_edge_index.size(1), 1) \n",
    "\n",
    "# [v] GO 계층 구조 edge 추가 (GO → GO)\n",
    "data[\"go\", \"is_a\", \"go\"].edge_index = go_hierarchy_edge_index\n",
    "data[\"go\", \"is_a\", \"go\"].edge_attr = torch.ones(go_hierarchy_edge_index.size(1), 1)\n",
    "\n",
    "#data[\"go\", \"rev_is_a\", \"go\"].edge_index = go_hierarchy_rev_edge_index\n",
    "#data[\"go\", \"rev_is_a\", \"go\"].edge_attr = torch.ones(go_hierarchy_rev_edge_index.size(1), 1) / 4\n",
    "\n",
    "# metadata (node type과 edge type) 자동 생성 혹은 명시적 정의\n",
    "metadata = (list(data.node_types), list(data.edge_types))\n",
    "print(\"노드 타입:\", data.node_types)\n",
    "print(\"엣지 타입:\", data.edge_types)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge index dictionary keys: dict_keys([('gene', 'PPI', 'gene'), ('gene', 'TOM', 'gene'), ('gene', 'has_GO', 'go'), ('go', 'is_a', 'go')])\n",
      "('gene', 'PPI', 'gene'): shape torch.Size([2, 44030])\n",
      "('gene', 'TOM', 'gene'): shape torch.Size([2, 143192])\n",
      "('gene', 'has_GO', 'go'): shape torch.Size([2, 146545])\n",
      "('go', 'is_a', 'go'): shape torch.Size([2, 29365])\n"
     ]
    }
   ],
   "source": [
    "print(\"Edge index dictionary keys:\", data.edge_index_dict.keys())\n",
    "for key, edge_index in data.edge_index_dict.items():\n",
    "    print(f\"{key}: shape {edge_index.shape}\")\n",
    "meta_paths = [\n",
    "    [('gene', 'PPI', 'gene')],\n",
    "    [('gene', 'TOM', 'gene')],\n",
    "    # 유전자와 GO 간 어노테이션 edge를 통한 경로: gene → has_GO → go → rev_has_GO → gene\n",
    "    #[('gene', 'has_GO', 'go'), ('go', 'rev_has_GO', 'gene')]#,\n",
    "    [('gene', 'has_GO', 'go')],\n",
    "    # (선택사항) GO 계층 구조를 포함한 경로: gene → has_GO → go → is_a → go → rev_has_GO → gene\n",
    "    [('go', 'is_a', 'go')]\n",
    "    #[('gene', 'has_GO', 'go'), ('go', 'is_a', 'go'), ('go', 'rev_has_GO', 'gene')]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a corruption function that shuffles node features.\n",
    "# This function now accepts two arguments: x_dict and edge_index_dict.\n",
    "\n",
    "def corruption(x_dict, edge_index_dict):\n",
    "    corrupted_x = {key: x[torch.randperm(x.size(0))] for key, x in x_dict.items() if x.size(0) > 1}  # ✅ 수정: 노드 수가 1개일 경우 permutation 오류 방지\n",
    "    return corrupted_x, edge_index_dict\n",
    "\n",
    "# Readout function (summarizing the whole graph representation)\n",
    "# Updated readout function to accept extra arguments.\n",
    "\n",
    "\n",
    "def readout(x, *args, **kwargs):\n",
    "    return torch.sigmoid(torch.mean(x, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HANEncoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 gene_in_dim,       # 초기 gene 특성 차원 (ex: 128)\n",
    "                 go_in_dim,         # 초기 go 특성 차원   (ex: 64)\n",
    "                 hidden_channels,\n",
    "                 out_channels,\n",
    "                 metadata,\n",
    "                 heads=8,\n",
    "                 dropout=0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # [1] 첫 번째 HANConv\n",
    "        in_channels_1 = {\n",
    "            'gene': gene_in_dim,\n",
    "            'go': go_in_dim\n",
    "        }\n",
    "        self.han_conv1 = HANConv(\n",
    "            in_channels_1, \n",
    "            hidden_channels,\n",
    "            metadata=metadata,\n",
    "            heads=heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # [2] 두 번째 Branch: TOM\n",
    "        #  - TOM도 같은 원본 x_dict를 입력으로 쓴다면, 차원은 동일\n",
    "        self.han_conv_tom = HANConv(\n",
    "            in_channels_1,\n",
    "            hidden_channels,\n",
    "            metadata=metadata,\n",
    "            heads=heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # [3] 세 번째 HANConv (두 Branch 결과 결합 후)\n",
    "        in_channels_2 = {\n",
    "            'gene': hidden_channels * 2,  # out_general + out_tom => 2배\n",
    "            'go': hidden_channels         # if 'go'는 한 가지만 살린다면 1배\n",
    "        }\n",
    "        self.han_conv2 = HANConv(\n",
    "            in_channels_2,\n",
    "            hidden_channels,\n",
    "            metadata=metadata,\n",
    "            heads=heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # [4] 최종 선형 변환: (hidden_channels * heads) -> out_channels\n",
    "        self.linear = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # 첫 번째 branch\n",
    "        out_general = self.han_conv1(x_dict, edge_index_dict)\n",
    "        \n",
    "        # 두 번째 branch(TOM) => edge_index_dict 중 TOM만 사용\n",
    "        tom_edge_index_dict = {\n",
    "            key: edge\n",
    "            for key, edge in edge_index_dict.items()\n",
    "            if key == ('gene', 'TOM', 'gene')\n",
    "        }\n",
    "        out_tom = self.han_conv_tom(x_dict, tom_edge_index_dict)\n",
    "        \n",
    "        # 두 branch 결합(gene)\n",
    "        merged_gene = torch.cat(\n",
    "            [out_general['gene'], out_tom['gene']],\n",
    "            dim=-1\n",
    "        )  # shape = [num_gene_nodes, 1024]\n",
    "        \n",
    "        # go 임베딩은 out_general['go']만 사용 (또는 tom도 있으면 cat)\n",
    "        merged_go = out_general['go']  # shape = [num_go_nodes, 512]\n",
    "        \n",
    "        # [5] 다음 레이어의 입력\n",
    "        merged_x_dict = {\n",
    "            'gene': merged_gene,\n",
    "            'go': merged_go\n",
    "        }\n",
    "        out_merge = self.han_conv2(merged_x_dict, edge_index_dict)\n",
    "        \n",
    "        # [6] 최종 gene 임베딩에 선형 변환\n",
    "        out_final = self.linear(out_merge['gene'])  # shape = [num_gene_nodes, out_channels]\n",
    "        \n",
    "        return out_final\n",
    "\n",
    "# in_channels는 각 노드 타입별 입력 feature dimension (one-hot feature이므로 node 개수)\n",
    "#in_channels = {'gene': num_genes + num_modules, 'go': num_go_nodes}\n",
    "in_channels = {'gene': num_modules + 2, 'go': num_go_features}\n",
    "hidden_dim = 64\n",
    "out_dim = 64\n",
    "heads = 8\n",
    "dropout = 0.3\n",
    "\n",
    "# metadata는 위에서 생성한 metadata 사용\n",
    "encoder = HANEncoder(\n",
    "    gene_in_dim=num_modules +2,\n",
    "    go_in_dim=num_go_features,\n",
    "    hidden_channels=hidden_dim,\n",
    "    out_channels=out_dim,\n",
    "    metadata=metadata,\n",
    "    heads=heads,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# DeepGraphInfomax 모델 구성\n",
    "dgi_model = DeepGraphInfomax(\n",
    "    hidden_channels=out_dim,\n",
    "    encoder=encoder,\n",
    "    summary=lambda x, *args, **kwargs: torch.sigmoid(torch.mean(x, dim=0)),  # readout 함수\n",
    "    corruption=lambda x_dict, edge_index_dict: (\n",
    "        {key: x[torch.randperm(x.size(0))] for key, x in x_dict.items()},\n",
    "        edge_index_dict\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Loss: 1.3938\n",
      "Epoch 01 | Loss: 1.3898\n",
      "Epoch 02 | Loss: 1.3866\n",
      "Epoch 03 | Loss: 1.3835\n",
      "Epoch 04 | Loss: 1.3798\n",
      "Epoch 05 | Loss: 1.3787\n",
      "Epoch 06 | Loss: 1.3752\n",
      "Epoch 07 | Loss: 1.3718\n",
      "Epoch 08 | Loss: 1.3690\n",
      "Epoch 09 | Loss: 1.3668\n",
      "Epoch 10 | Loss: 1.3643\n",
      "Epoch 11 | Loss: 1.3621\n",
      "Epoch 12 | Loss: 1.3593\n",
      "Epoch 13 | Loss: 1.3568\n",
      "Epoch 14 | Loss: 1.3539\n",
      "Epoch 15 | Loss: 1.3511\n",
      "Epoch 16 | Loss: 1.3500\n",
      "Epoch 17 | Loss: 1.3443\n",
      "Epoch 18 | Loss: 1.3405\n",
      "Epoch 19 | Loss: 1.3409\n",
      "Epoch 20 | Loss: 1.3344\n",
      "Epoch 21 | Loss: 1.3291\n",
      "Epoch 22 | Loss: 1.3257\n",
      "Epoch 23 | Loss: 1.3225\n",
      "Epoch 24 | Loss: 1.3191\n",
      "Epoch 25 | Loss: 1.3135\n",
      "Epoch 26 | Loss: 1.3080\n",
      "Epoch 27 | Loss: 1.3090\n",
      "Epoch 28 | Loss: 1.3003\n",
      "Epoch 29 | Loss: 1.2996\n",
      "Epoch 30 | Loss: 1.2942\n",
      "Epoch 31 | Loss: 1.2856\n",
      "Epoch 32 | Loss: 1.2822\n",
      "Epoch 33 | Loss: 1.2788\n",
      "Epoch 34 | Loss: 1.2749\n",
      "Epoch 35 | Loss: 1.2607\n",
      "Epoch 36 | Loss: 1.2603\n",
      "Epoch 37 | Loss: 1.2508\n",
      "Epoch 38 | Loss: 1.2463\n",
      "Epoch 39 | Loss: 1.2340\n",
      "Epoch 40 | Loss: 1.2314\n",
      "Epoch 41 | Loss: 1.2306\n",
      "Epoch 42 | Loss: 1.2174\n",
      "Epoch 43 | Loss: 1.2000\n",
      "Epoch 44 | Loss: 1.1988\n",
      "Epoch 45 | Loss: 1.1926\n",
      "Epoch 46 | Loss: 1.1723\n",
      "Epoch 47 | Loss: 1.1747\n",
      "Epoch 48 | Loss: 1.1656\n",
      "Epoch 49 | Loss: 1.1706\n",
      "Epoch 50 | Loss: 1.1526\n",
      "Epoch 51 | Loss: 1.1447\n",
      "Epoch 52 | Loss: 1.1391\n",
      "Epoch 53 | Loss: 1.1031\n",
      "Epoch 54 | Loss: 1.1178\n",
      "Epoch 55 | Loss: 1.1097\n",
      "Epoch 56 | Loss: 1.1048\n",
      "Epoch 57 | Loss: 1.0809\n",
      "Epoch 58 | Loss: 1.0863\n",
      "Epoch 59 | Loss: 1.0593\n",
      "Epoch 60 | Loss: 1.0563\n",
      "Epoch 61 | Loss: 1.0588\n",
      "Epoch 62 | Loss: 1.0109\n",
      "Epoch 63 | Loss: 1.0040\n",
      "Epoch 64 | Loss: 0.9938\n",
      "Epoch 65 | Loss: 0.9829\n",
      "Epoch 66 | Loss: 0.9738\n",
      "Epoch 67 | Loss: 0.9728\n",
      "Epoch 68 | Loss: 0.9406\n",
      "Epoch 69 | Loss: 0.9398\n",
      "Epoch 70 | Loss: 0.9224\n",
      "Epoch 71 | Loss: 0.9093\n",
      "Epoch 72 | Loss: 0.8942\n",
      "Epoch 73 | Loss: 0.8870\n",
      "Epoch 74 | Loss: 0.8744\n",
      "Epoch 75 | Loss: 0.8864\n",
      "Epoch 76 | Loss: 0.8596\n",
      "Epoch 77 | Loss: 0.8485\n",
      "Epoch 78 | Loss: 0.8321\n",
      "Epoch 79 | Loss: 0.8191\n",
      "Epoch 80 | Loss: 0.7978\n",
      "Epoch 81 | Loss: 0.8036\n",
      "Epoch 82 | Loss: 0.7677\n",
      "Epoch 83 | Loss: 0.7632\n",
      "Epoch 84 | Loss: 0.7644\n",
      "Epoch 85 | Loss: 0.7670\n",
      "Epoch 86 | Loss: 0.7523\n",
      "Epoch 87 | Loss: 0.7271\n",
      "Epoch 88 | Loss: 0.7056\n",
      "Epoch 89 | Loss: 0.7277\n",
      "Epoch 90 | Loss: 0.7165\n",
      "Epoch 91 | Loss: 0.7039\n",
      "Epoch 92 | Loss: 0.7196\n",
      "Epoch 93 | Loss: 0.6652\n",
      "Epoch 94 | Loss: 0.6741\n",
      "Epoch 95 | Loss: 0.6627\n",
      "Epoch 96 | Loss: 0.6356\n",
      "Epoch 97 | Loss: 0.6663\n",
      "Epoch 98 | Loss: 0.6263\n",
      "Epoch 99 | Loss: 0.6165\n",
      "Epoch 100 | Loss: 0.5906\n",
      "Epoch 101 | Loss: 0.6182\n",
      "Epoch 102 | Loss: 0.5791\n",
      "Epoch 103 | Loss: 0.6130\n",
      "Epoch 104 | Loss: 0.5666\n",
      "Epoch 105 | Loss: 0.5517\n",
      "Epoch 106 | Loss: 0.5609\n",
      "Epoch 107 | Loss: 0.5412\n",
      "Epoch 108 | Loss: 0.5542\n",
      "Epoch 109 | Loss: 0.5366\n",
      "Epoch 110 | Loss: 0.5269\n",
      "Epoch 111 | Loss: 0.5117\n",
      "Epoch 112 | Loss: 0.5228\n",
      "Epoch 113 | Loss: 0.5382\n",
      "Epoch 114 | Loss: 0.5032\n",
      "Epoch 115 | Loss: 0.4828\n",
      "Epoch 116 | Loss: 0.4824\n",
      "Epoch 117 | Loss: 0.4849\n",
      "Epoch 118 | Loss: 0.4955\n",
      "Epoch 119 | Loss: 0.4750\n",
      "Epoch 120 | Loss: 0.4647\n",
      "Epoch 121 | Loss: 0.4537\n",
      "Epoch 122 | Loss: 0.4652\n",
      "Epoch 123 | Loss: 0.4676\n",
      "Epoch 124 | Loss: 0.4454\n",
      "Epoch 125 | Loss: 0.4381\n",
      "Epoch 126 | Loss: 0.4475\n",
      "Epoch 127 | Loss: 0.4218\n",
      "Epoch 128 | Loss: 0.4445\n",
      "Epoch 129 | Loss: 0.4482\n",
      "Epoch 130 | Loss: 0.4186\n",
      "Epoch 131 | Loss: 0.4172\n",
      "Epoch 132 | Loss: 0.4245\n",
      "Epoch 133 | Loss: 0.3956\n",
      "Epoch 134 | Loss: 0.3962\n",
      "Epoch 135 | Loss: 0.3783\n",
      "Epoch 136 | Loss: 0.4151\n",
      "Epoch 137 | Loss: 0.4154\n",
      "Epoch 138 | Loss: 0.3915\n",
      "Epoch 139 | Loss: 0.3797\n",
      "Epoch 140 | Loss: 0.3741\n",
      "Epoch 141 | Loss: 0.3763\n",
      "Epoch 142 | Loss: 0.4055\n",
      "Epoch 143 | Loss: 0.3669\n",
      "Epoch 144 | Loss: 0.3666\n",
      "Epoch 145 | Loss: 0.3701\n",
      "Epoch 146 | Loss: 0.3792\n",
      "Epoch 147 | Loss: 0.3613\n",
      "Epoch 148 | Loss: 0.3805\n",
      "Epoch 149 | Loss: 0.3551\n",
      "Epoch 150 | Loss: 0.3467\n",
      "Epoch 151 | Loss: 0.3627\n",
      "Epoch 152 | Loss: 0.3356\n",
      "Epoch 153 | Loss: 0.3288\n",
      "Epoch 154 | Loss: 0.3277\n",
      "Epoch 155 | Loss: 0.3407\n",
      "Epoch 156 | Loss: 0.3554\n",
      "Epoch 157 | Loss: 0.3752\n",
      "Epoch 158 | Loss: 0.3450\n",
      "Epoch 159 | Loss: 0.3214\n",
      "Epoch 160 | Loss: 0.3224\n",
      "Epoch 161 | Loss: 0.3326\n",
      "Epoch 162 | Loss: 0.3310\n",
      "Epoch 163 | Loss: 0.3199\n",
      "Epoch 164 | Loss: 0.3405\n",
      "Epoch 165 | Loss: 0.3247\n",
      "Epoch 166 | Loss: 0.3188\n",
      "Epoch 167 | Loss: 0.3580\n",
      "Epoch 168 | Loss: 0.3032\n",
      "Epoch 169 | Loss: 0.3283\n",
      "Epoch 170 | Loss: 0.3321\n",
      "Epoch 171 | Loss: 0.3446\n",
      "Epoch 172 | Loss: 0.3174\n",
      "Epoch 173 | Loss: 0.3006\n",
      "Epoch 174 | Loss: 0.3074\n",
      "Epoch 175 | Loss: 0.2932\n",
      "Epoch 176 | Loss: 0.3093\n",
      "Epoch 177 | Loss: 0.2950\n",
      "Epoch 178 | Loss: 0.2925\n",
      "Epoch 179 | Loss: 0.2910\n",
      "Epoch 180 | Loss: 0.3065\n",
      "Epoch 181 | Loss: 0.3035\n",
      "Epoch 182 | Loss: 0.2954\n",
      "Epoch 183 | Loss: 0.3033\n",
      "Epoch 184 | Loss: 0.2829\n",
      "Epoch 185 | Loss: 0.2843\n",
      "Epoch 186 | Loss: 0.3234\n",
      "Epoch 187 | Loss: 0.3249\n",
      "Epoch 188 | Loss: 0.3004\n",
      "Epoch 189 | Loss: 0.2811\n",
      "Epoch 190 | Loss: 0.2993\n",
      "Epoch 191 | Loss: 0.2892\n",
      "Epoch 192 | Loss: 0.2767\n",
      "Epoch 193 | Loss: 0.3008\n",
      "Epoch 194 | Loss: 0.2821\n",
      "Epoch 195 | Loss: 0.2765\n",
      "Epoch 196 | Loss: 0.2712\n",
      "Epoch 197 | Loss: 0.2939\n",
      "Epoch 198 | Loss: 0.2677\n",
      "Epoch 199 | Loss: 0.2717\n"
     ]
    }
   ],
   "source": [
    "## train module\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "dgi_model = dgi_model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(dgi_model.parameters(), lr=0.0002)\n",
    "\n",
    "def train_dgi(model, data, optimizer, epochs=200):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        pos_z, neg_z, summary = model(data.x_dict, data.edge_index_dict)\n",
    "        loss = model.loss(pos_z, neg_z, summary)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch:02d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "train_dgi(dgi_model, data, optimizer, epochs=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_prefix = \"PPI_TOM_Module_eQTL/dgi_model_HAN_2layers_hierarchy_lr2e4_200epoch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n",
      "Embeddings is not a dict, using entire tensor.\n",
      "Gene Embeddings Shape: (2615, 64)\n"
     ]
    }
   ],
   "source": [
    "# Save model state dict\n",
    "#torch.save(dgi_model.state_dict(), \"../analysis/assets/module_assignment_network/\"+file_prefix+\".pth\")\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = dgi_model.encoder(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "# Check the structure of embeddings\n",
    "if isinstance(embeddings, dict):\n",
    "    print(\"Embeddings keys:\", embeddings.keys())  # Should include \"gene\"\n",
    "    \n",
    "    if \"gene\" in embeddings:\n",
    "        print(\"Embeddings['gene'] shape:\", embeddings[\"gene\"].shape)\n",
    "        gene_embeddings = embeddings[\"gene\"].cpu().numpy()\n",
    "    else:\n",
    "        raise ValueError(\"Key 'gene' not found in embeddings.\")\n",
    "        \n",
    "else:  # If embeddings is a single tensor, no need to index with 'gene'\n",
    "    print(\"Embeddings is not a dict, using entire tensor.\")\n",
    "    gene_embeddings = embeddings.cpu().numpy()\n",
    "\n",
    "print(\"Gene Embeddings Shape:\", gene_embeddings.shape)  # Should be (num_nodes, hidden_dim)\n",
    "\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "G = nx.Graph()\n",
    "\n",
    "node_names = {\n",
    "    \"gene\": data[\"gene\"].names#,  # Assuming node names are stored\n",
    "    #\"go\": data[\"go\"].names\n",
    "}\n",
    "# Convert torch_geometric graph to NetworkX\n",
    "\n",
    "# Add nodes to NetworkX with labels\n",
    "for node_type, names in node_names.items():\n",
    "    for i, name in enumerate(names):\n",
    "        G.add_node(f\"{node_type}_{i}\", type=node_type, name=name)\n",
    "\n",
    "# Convert edges\n",
    "for edge_type, edge_index in data.edge_index_dict.items():\n",
    "    src_type, relation, dst_type = edge_type\n",
    "    edge_index = edge_index.cpu().numpy()\n",
    "    # Weight 추가 (TOM/PPI/GO 관계마다 다를 수 있음)\n",
    "    edge_weights = data[edge_type].edge_attr.cpu().numpy().flatten()\n",
    "\n",
    "    for (src, dst, weight) in zip(edge_index[0], edge_index[1], edge_weights):\n",
    "        G.add_edge(f\"{src_type}_{src}\", f\"{dst_type}_{dst}\", relation=relation, weight=weight)\n",
    "\n",
    "# Save as edge list (CSV format)\n",
    "#edge_list = nx.to_pandas_edgelist(G)\n",
    "#edge_list.to_csv(\"../analysis/assets/module_assignment_network/PPI_TOM_eQTL/dgi_model_HAN_2layers_lr2e4_200epoch.csv\", index=False)\n",
    "\n",
    "# Save as GraphML or GML for direct import into R\n",
    "#nx.write_graphml(G, \"../analysis/assets/module_assignment_network/\"+file_prefix+\".graphml\")\n",
    "\n",
    "df_gene_embeddings = pd.DataFrame(gene_embeddings)\n",
    "df_gene_embeddings.index = data[\"gene\"].names\n",
    "#df_gene_embeddings.to_csv(\"../analysis/assets/module_assignment_network/\"+file_prefix+\".tsv.gz\", sep = \"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_df.to_csv(\"../analysis/assets/module_assignment_network/\"+file_prefix+\"_moduleembedding.tsv.gz\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: 0.2792, p-value: 4.8748e-48\n"
     ]
    }
   ],
   "source": [
    "from networkx.algorithms.centrality import eigenvector_centrality\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "for edge in ppi_edge_index.T.tolist():\n",
    "    G.add_edge(common_genes[edge[0]], common_genes[edge[1]])\n",
    "\n",
    "ec = eigenvector_centrality(G)\n",
    "ec_values = np.array([ec[gene] if gene in ec else 0 for gene in data[\"gene\"].names])\n",
    "\n",
    "\n",
    "# 비교\n",
    "import scipy.stats\n",
    "correlation, pval = scipy.stats.spearmanr(ec_values, np.linalg.norm(gene_embeddings, axis=1))\n",
    "print(f\"Spearman correlation: {correlation:.4f}, p-value: {pval:.4e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_genes = module_df[module_df[\"module\"] == \"brown\"][\"gene_name\"].values\n",
    "module_embeddings = gene_embeddings[np.isin(data[\"gene\"].names, module_genes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ABHD17A</th>\n",
       "      <td>-0.300179</td>\n",
       "      <td>0.137569</td>\n",
       "      <td>0.261652</td>\n",
       "      <td>-0.075853</td>\n",
       "      <td>0.166381</td>\n",
       "      <td>0.131724</td>\n",
       "      <td>0.254669</td>\n",
       "      <td>-0.183976</td>\n",
       "      <td>0.336819</td>\n",
       "      <td>-0.149537</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145318</td>\n",
       "      <td>-0.206203</td>\n",
       "      <td>0.199884</td>\n",
       "      <td>-0.097787</td>\n",
       "      <td>0.015048</td>\n",
       "      <td>-0.246346</td>\n",
       "      <td>-0.191160</td>\n",
       "      <td>-0.007187</td>\n",
       "      <td>0.154420</td>\n",
       "      <td>-0.026405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABI3</th>\n",
       "      <td>-0.441602</td>\n",
       "      <td>0.274306</td>\n",
       "      <td>0.346450</td>\n",
       "      <td>-0.246595</td>\n",
       "      <td>0.314622</td>\n",
       "      <td>0.383928</td>\n",
       "      <td>0.434003</td>\n",
       "      <td>-0.285416</td>\n",
       "      <td>0.637197</td>\n",
       "      <td>-0.295935</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.255691</td>\n",
       "      <td>-0.382908</td>\n",
       "      <td>0.423442</td>\n",
       "      <td>-0.301198</td>\n",
       "      <td>-0.141167</td>\n",
       "      <td>-0.457048</td>\n",
       "      <td>-0.323813</td>\n",
       "      <td>-0.041822</td>\n",
       "      <td>0.222234</td>\n",
       "      <td>-0.147580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACAA2</th>\n",
       "      <td>-0.264870</td>\n",
       "      <td>0.118533</td>\n",
       "      <td>0.240402</td>\n",
       "      <td>-0.062944</td>\n",
       "      <td>0.156140</td>\n",
       "      <td>0.126572</td>\n",
       "      <td>0.253285</td>\n",
       "      <td>-0.180750</td>\n",
       "      <td>0.305656</td>\n",
       "      <td>-0.159245</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.140336</td>\n",
       "      <td>-0.203008</td>\n",
       "      <td>0.199060</td>\n",
       "      <td>-0.097091</td>\n",
       "      <td>0.019084</td>\n",
       "      <td>-0.225547</td>\n",
       "      <td>-0.185464</td>\n",
       "      <td>0.010413</td>\n",
       "      <td>0.156449</td>\n",
       "      <td>-0.041054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACTB</th>\n",
       "      <td>-0.294920</td>\n",
       "      <td>0.142536</td>\n",
       "      <td>0.251707</td>\n",
       "      <td>-0.094206</td>\n",
       "      <td>0.195031</td>\n",
       "      <td>0.187429</td>\n",
       "      <td>0.299303</td>\n",
       "      <td>-0.188728</td>\n",
       "      <td>0.357577</td>\n",
       "      <td>-0.209110</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.174392</td>\n",
       "      <td>-0.252127</td>\n",
       "      <td>0.257003</td>\n",
       "      <td>-0.146155</td>\n",
       "      <td>-0.011180</td>\n",
       "      <td>-0.273348</td>\n",
       "      <td>-0.219319</td>\n",
       "      <td>0.022049</td>\n",
       "      <td>0.173095</td>\n",
       "      <td>-0.053520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACTN4</th>\n",
       "      <td>-0.295488</td>\n",
       "      <td>0.173578</td>\n",
       "      <td>0.259273</td>\n",
       "      <td>-0.086216</td>\n",
       "      <td>0.211026</td>\n",
       "      <td>0.231656</td>\n",
       "      <td>0.339648</td>\n",
       "      <td>-0.199336</td>\n",
       "      <td>0.384944</td>\n",
       "      <td>-0.241595</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196666</td>\n",
       "      <td>-0.298019</td>\n",
       "      <td>0.271843</td>\n",
       "      <td>-0.169481</td>\n",
       "      <td>-0.024549</td>\n",
       "      <td>-0.277458</td>\n",
       "      <td>-0.220702</td>\n",
       "      <td>0.010418</td>\n",
       "      <td>0.198573</td>\n",
       "      <td>-0.096684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZDHHC24</th>\n",
       "      <td>-0.203623</td>\n",
       "      <td>0.065864</td>\n",
       "      <td>0.191002</td>\n",
       "      <td>-0.002175</td>\n",
       "      <td>0.096693</td>\n",
       "      <td>0.072215</td>\n",
       "      <td>0.212821</td>\n",
       "      <td>-0.153083</td>\n",
       "      <td>0.227330</td>\n",
       "      <td>-0.146388</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108034</td>\n",
       "      <td>-0.182625</td>\n",
       "      <td>0.123329</td>\n",
       "      <td>-0.033790</td>\n",
       "      <td>0.035153</td>\n",
       "      <td>-0.153209</td>\n",
       "      <td>-0.175157</td>\n",
       "      <td>0.036870</td>\n",
       "      <td>0.148863</td>\n",
       "      <td>-0.017272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZEB2</th>\n",
       "      <td>-0.267998</td>\n",
       "      <td>0.125473</td>\n",
       "      <td>0.234346</td>\n",
       "      <td>-0.076397</td>\n",
       "      <td>0.146242</td>\n",
       "      <td>0.119711</td>\n",
       "      <td>0.264548</td>\n",
       "      <td>-0.186966</td>\n",
       "      <td>0.291843</td>\n",
       "      <td>-0.168485</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.153481</td>\n",
       "      <td>-0.195600</td>\n",
       "      <td>0.230783</td>\n",
       "      <td>-0.117108</td>\n",
       "      <td>0.011910</td>\n",
       "      <td>-0.241102</td>\n",
       "      <td>-0.192241</td>\n",
       "      <td>0.051686</td>\n",
       "      <td>0.148377</td>\n",
       "      <td>0.002529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF428</th>\n",
       "      <td>-0.202765</td>\n",
       "      <td>0.074519</td>\n",
       "      <td>0.196145</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>0.101539</td>\n",
       "      <td>0.083075</td>\n",
       "      <td>0.227929</td>\n",
       "      <td>-0.150659</td>\n",
       "      <td>0.235697</td>\n",
       "      <td>-0.156353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111158</td>\n",
       "      <td>-0.197385</td>\n",
       "      <td>0.125020</td>\n",
       "      <td>-0.041884</td>\n",
       "      <td>0.028598</td>\n",
       "      <td>-0.155256</td>\n",
       "      <td>-0.178723</td>\n",
       "      <td>0.038162</td>\n",
       "      <td>0.154811</td>\n",
       "      <td>-0.025718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF683</th>\n",
       "      <td>-0.200033</td>\n",
       "      <td>0.067443</td>\n",
       "      <td>0.190725</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.092179</td>\n",
       "      <td>0.075509</td>\n",
       "      <td>0.217757</td>\n",
       "      <td>-0.149135</td>\n",
       "      <td>0.228258</td>\n",
       "      <td>-0.149131</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109148</td>\n",
       "      <td>-0.186305</td>\n",
       "      <td>0.120590</td>\n",
       "      <td>-0.036816</td>\n",
       "      <td>0.033762</td>\n",
       "      <td>-0.150016</td>\n",
       "      <td>-0.177631</td>\n",
       "      <td>0.042280</td>\n",
       "      <td>0.149690</td>\n",
       "      <td>-0.017637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNHIT1</th>\n",
       "      <td>-0.207338</td>\n",
       "      <td>0.093002</td>\n",
       "      <td>0.205344</td>\n",
       "      <td>-0.025857</td>\n",
       "      <td>0.111964</td>\n",
       "      <td>0.079889</td>\n",
       "      <td>0.218228</td>\n",
       "      <td>-0.147520</td>\n",
       "      <td>0.209061</td>\n",
       "      <td>-0.148474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127536</td>\n",
       "      <td>-0.192507</td>\n",
       "      <td>0.151635</td>\n",
       "      <td>-0.065721</td>\n",
       "      <td>0.058727</td>\n",
       "      <td>-0.180988</td>\n",
       "      <td>-0.160101</td>\n",
       "      <td>0.012802</td>\n",
       "      <td>0.151332</td>\n",
       "      <td>-0.000143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6   \\\n",
       "ABHD17A -0.300179  0.137569  0.261652 -0.075853  0.166381  0.131724  0.254669   \n",
       "ABI3    -0.441602  0.274306  0.346450 -0.246595  0.314622  0.383928  0.434003   \n",
       "ACAA2   -0.264870  0.118533  0.240402 -0.062944  0.156140  0.126572  0.253285   \n",
       "ACTB    -0.294920  0.142536  0.251707 -0.094206  0.195031  0.187429  0.299303   \n",
       "ACTN4   -0.295488  0.173578  0.259273 -0.086216  0.211026  0.231656  0.339648   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "ZDHHC24 -0.203623  0.065864  0.191002 -0.002175  0.096693  0.072215  0.212821   \n",
       "ZEB2    -0.267998  0.125473  0.234346 -0.076397  0.146242  0.119711  0.264548   \n",
       "ZNF428  -0.202765  0.074519  0.196145  0.003616  0.101539  0.083075  0.227929   \n",
       "ZNF683  -0.200033  0.067443  0.190725  0.005025  0.092179  0.075509  0.217757   \n",
       "ZNHIT1  -0.207338  0.093002  0.205344 -0.025857  0.111964  0.079889  0.218228   \n",
       "\n",
       "               7         8         9   ...        54        55        56  \\\n",
       "ABHD17A -0.183976  0.336819 -0.149537  ... -0.145318 -0.206203  0.199884   \n",
       "ABI3    -0.285416  0.637197 -0.295935  ... -0.255691 -0.382908  0.423442   \n",
       "ACAA2   -0.180750  0.305656 -0.159245  ... -0.140336 -0.203008  0.199060   \n",
       "ACTB    -0.188728  0.357577 -0.209110  ... -0.174392 -0.252127  0.257003   \n",
       "ACTN4   -0.199336  0.384944 -0.241595  ... -0.196666 -0.298019  0.271843   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "ZDHHC24 -0.153083  0.227330 -0.146388  ... -0.108034 -0.182625  0.123329   \n",
       "ZEB2    -0.186966  0.291843 -0.168485  ... -0.153481 -0.195600  0.230783   \n",
       "ZNF428  -0.150659  0.235697 -0.156353  ... -0.111158 -0.197385  0.125020   \n",
       "ZNF683  -0.149135  0.228258 -0.149131  ... -0.109148 -0.186305  0.120590   \n",
       "ZNHIT1  -0.147520  0.209061 -0.148474  ... -0.127536 -0.192507  0.151635   \n",
       "\n",
       "               57        58        59        60        61        62        63  \n",
       "ABHD17A -0.097787  0.015048 -0.246346 -0.191160 -0.007187  0.154420 -0.026405  \n",
       "ABI3    -0.301198 -0.141167 -0.457048 -0.323813 -0.041822  0.222234 -0.147580  \n",
       "ACAA2   -0.097091  0.019084 -0.225547 -0.185464  0.010413  0.156449 -0.041054  \n",
       "ACTB    -0.146155 -0.011180 -0.273348 -0.219319  0.022049  0.173095 -0.053520  \n",
       "ACTN4   -0.169481 -0.024549 -0.277458 -0.220702  0.010418  0.198573 -0.096684  \n",
       "...           ...       ...       ...       ...       ...       ...       ...  \n",
       "ZDHHC24 -0.033790  0.035153 -0.153209 -0.175157  0.036870  0.148863 -0.017272  \n",
       "ZEB2    -0.117108  0.011910 -0.241102 -0.192241  0.051686  0.148377  0.002529  \n",
       "ZNF428  -0.041884  0.028598 -0.155256 -0.178723  0.038162  0.154811 -0.025718  \n",
       "ZNF683  -0.036816  0.033762 -0.150016 -0.177631  0.042280  0.149690 -0.017637  \n",
       "ZNHIT1  -0.065721  0.058727 -0.180988 -0.160101  0.012802  0.151332 -0.000143  \n",
       "\n",
       "[501 rows x 64 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(module_embeddings, index = module_genes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg-cu124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
